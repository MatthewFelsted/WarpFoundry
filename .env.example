# Copy this file to .env and fill in your API keys.
# NEVER commit .env to version control.

OPENAI_API_KEY=sk-proj-your-key-here
ANTHROPIC_API_KEY=sk-ant-your-key-here
GOOGLE_API_KEY=your-google-api-key-here
XAI_API_KEY=xai-your-key-here

# ── Local Ollama Setup ──────────────────────────────────────────
# Ollama lets you run models locally — completely free, offline, private.
#
# Quick start:
#   1. Install Ollama:  https://ollama.com/download
#   2. Pull a model:    ollama pull gemma3:27b
#   3. Start server:    ollama serve
#   4. In the GUI, check "Local Only" or use --local-only on the CLI
#
# The manager auto-detects installed Ollama models. The GUI brain model
# dropdown dynamically shows whatever you have pulled.
#
OLLAMA_BASE_URL=http://localhost:11434
# OLLAMA_AUTO_START=false    # Set to true to auto-start Ollama if not running

# ── CUA (Computer-Using Agent) Setup ────────────────────────────
# CUA lets an AI visually test your app by controlling a browser.
# It takes screenshots, clicks buttons, fills forms, and reports issues.
#
# Requires:
#   pip install codex-manager[cua]
#   python -m playwright install            (installs Chromium + headless shell; use "install" not just "install chromium")
#
# CUA uses OPENAI_API_KEY (for OpenAI CUA) or ANTHROPIC_API_KEY (for Claude).
# No additional keys needed beyond those already set above.
#
# Usage:
#   CLI:   python -m codex_manager visual-test --url http://localhost:5088
#   GUI:   Enable "Computer-Use Agent" in Pipeline config
#   Pipeline: Enable CUA in pipeline config → visual_test phase runs automatically
#
# CUA_DEFAULT_PROVIDER=openai     # "openai" or "anthropic"
# CUA_HEADLESS=true               # Set to false to see the browser in action
# CUA_OPENAI_MODEL=computer-use-preview   # Override if needed; computer-use requires API Tier 3+ or allowlist

# ── Brain model preferences ─────────────────────────────────────
# Used by AI Manager's thinking layer (brain, optimizer, scientist).
# When "Local Only" is enabled (GUI toggle or --local-only), these
# are overridden to use your first installed Ollama model.
#
# AI_LEAD_MODEL=gpt-5.2
# AI_CHEAP_MODEL=grok-4-1-fast-reasoning
# AI_MEDIUM_MODEL=gpt-5.2
# AI_FREE_MODEL=ollama:gemma3:27b
